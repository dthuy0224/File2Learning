# ğŸ“Š Dataset Collection Guide - Difficulty Classification

TiÃªu chÃ­ vÃ  hÆ°á»›ng dáº«n tÃ¬m dataset cháº¥t lÆ°á»£ng cho training AI model

---

## ğŸ¯ TiÃªu ChÃ­ Dataset Cháº¥t LÆ°á»£ng

### 1. **Minimum Requirements (Báº®T BUá»˜C)**

#### âœ… Size (KÃ­ch thÆ°á»›c)
```
Minimum:  1,000 samples  (cÃ³ thá»ƒ train, accuracy ~75%)
Good:     5,000 samples  (accuracy ~85%)
Excellent: 10,000+ samples (accuracy ~90%+)

Per-label distribution:
- A1: â‰¥ 150 samples
- A2: â‰¥ 150 samples  
- B1: â‰¥ 200 samples (most common level)
- B2: â‰¥ 200 samples
- C1: â‰¥ 150 samples
- C2: â‰¥ 150 samples
```

**Why?** 
- BERT models cáº§n Ã­t nháº¥t 100 examples/class Ä‘á»ƒ há»c patterns
- Balanced dataset â†’ better generalization

---

#### âœ… Labels (NhÃ£n)
```
REQUIRED: CEFR levels (A1, A2, B1, B2, C1, C2)

Acceptable alternatives:
- Readability scores (Flesch-Kincaid) â†’ convert to CEFR
- Grade levels (Grade 1-12) â†’ map to CEFR
- Lexile scores (200L-1700L) â†’ convert to CEFR

AVOID:
âŒ "Easy/Medium/Hard" (too vague)
âŒ Unlabeled data (cáº§n manual labeling)
âŒ Domain-specific only (medical, legal) â†’ not general
```

**Conversion table:**
```
Grade Level â†’ CEFR
1-2    â†’ A1
3-4    â†’ A2
5-6    â†’ B1
7-8    â†’ B2
9-10   â†’ C1
11-12  â†’ C2

Lexile â†’ CEFR
200-400L   â†’ A1
400-600L   â†’ A2
600-900L   â†’ B1
900-1100L  â†’ B2
1100-1300L â†’ C1
1300L+     â†’ C2
```

---

#### âœ… Quality (Cháº¥t lÆ°á»£ng content)
```
Good text characteristics:
âœ… Natural English (not machine-translated)
âœ… Complete sentences
âœ… Proper grammar
âœ… Clear meaning
âœ… Appropriate length (50-500 words/sample)

Red flags:
âŒ OCR errors (scanned documents with typos)
âŒ Code-switching (mixed languages)
âŒ Too short (< 20 words)
âŒ Too long (> 1000 words) â†’ hard to label
âŒ Machine-generated gibberish
```

---

#### âœ… Diversity (Äa dáº¡ng)
```
Topic diversity:
âœ… News articles
âœ… Stories/narratives
âœ… Educational texts
âœ… Social media (informal)
âœ… Academic papers (formal)
âœ… Conversations/dialogues

Genre balance:
- Fiction: 30%
- Non-fiction: 40%
- Academic: 20%
- Informal: 10%

AVOID: Single-domain datasets (all medical, all legal)
```

---

### 2. **Nice-to-Have Features (KHUYáº¾N NGHá»Š)**

#### ğŸŒŸ Metadata
```
Useful metadata fields:
- Source (news, textbook, social media)
- Topic/category
- Author information
- Publication date
- Word count
- Readability metrics
- Target audience (kids, adults, professionals)
```

#### ğŸŒŸ Pre-processed
```
Ideal format:
{
  "text": "The actual English text...",
  "label": "B1",
  "metadata": {
    "source": "news",
    "topic": "environment",
    "word_count": 250,
    "flesch_score": 65.2
  }
}

Saves time vs raw text files!
```

#### ğŸŒŸ Train/Val/Test Splits
```
Pre-split datasets = time saver!

Look for:
- train.json
- val.json  
- test.json

With stratified sampling (balanced labels)
```

---

## ğŸ” Recommended Dataset Sources

### **Tier 1: FREE & HIGH-QUALITY** â­â­â­â­â­

#### 1. **OneStopEnglish Corpus**
```
URL: https://github.com/nishkalavallabhi/OneStopEnglishCorpus
Size: 567 texts (189 articles Ã— 3 levels)
Labels: Elementary (A2), Intermediate (B1), Advanced (C1)
Format: Text files
License: CC BY-SA 4.0 (Free for academic)

Pros:
âœ… Same article at 3 difficulty levels
âœ… News articles (Guardian newspaper)
âœ… High quality, manually adapted
âœ… Academic research standard

Cons:
âš ï¸ Only 3 levels (missing A1, B2, C2)
âš ï¸ All news domain
âš ï¸ Need manual processing

Download:
git clone https://github.com/nishkalavallabhi/OneStopEnglishCorpus.git
```

---

#### 2. **NewsELA Dataset**
```
URL: https://newsela.com/data/
Size: 1,000+ articles Ã— 5 levels
Labels: Lexile scores â†’ convert to CEFR
Format: HTML/Text
License: Academic use (free with request)

Pros:
âœ… 5 reading levels
âœ… Large scale
âœ… Current news topics
âœ… Used in research

Cons:
âš ï¸ Need to request access
âš ï¸ Web scraping required
âš ï¸ Lexile â†’ CEFR conversion needed

Access: Fill form at newsela.com/data
```

---

#### 3. **Cambridge English Corpus**
```
URL: https://www.cambridge.org/gb/cambridgeenglish/
Size: 5,000+ texts with CEFR labels
Labels: Official CEFR (A1-C2) âœ…
Format: Various
License: Academic/Research license

Pros:
âœ… OFFICIAL CEFR labels (gold standard)
âœ… All 6 levels
âœ… Exam-quality texts
âœ… Multiple genres

Cons:
âš ï¸ Need to apply for access
âš ï¸ May have restrictions
âš ï¸ Not all publicly available

Contact: Cambridge Assessment English
```

---

#### 4. **CEFR-J Corpus (Japanese Learners)**
```
URL: http://www.cefr-j.org/
Size: 2,000+ texts
Labels: CEFR (A1-C2)
Format: Text files
License: Research use

Pros:
âœ… CEFR labeled
âœ… Educational focus
âœ… Free for research

Cons:
âš ï¸ Focused on Japanese learners
âš ï¸ May need translation
```

---

#### 5. **CommonLit Corpus**
```
URL: https://www.commonlit.org/
Size: 2,000+ reading passages
Labels: Grade levels (3-12) â†’ convert to CEFR
Format: Web (can scrape)
License: Educational use

Pros:
âœ… High-quality educational texts
âœ… Various genres
âœ… Comprehension questions included
âœ… Free for teachers

Cons:
âš ï¸ Need to scrape/download
âš ï¸ Grade level â†’ CEFR conversion
```

---

### **Tier 2: GOOD ALTERNATIVES** â­â­â­â­

#### 6. **Wikipedia Simple English**
```
URL: https://simple.wikipedia.org/
Size: 200,000+ articles
Labels: Binary (Simple vs Normal)
Format: Wiki dumps
License: CC (Free)

Pros:
âœ… Huge dataset
âœ… Free & accessible
âœ… Easy to download

Cons:
âš ï¸ Only 2 levels (not full CEFR)
âš ï¸ Need manual CEFR labeling
âš ï¸ Quality varies

Use case: Augment dataset with A2 (simple) and C1 (normal)
```

---

#### 7. **Oxford Graded Readers**
```
URL: Various bookstores/libraries
Size: 1,000+ graded books
Labels: Oxford Bookworms levels (1-6)
Format: Books
License: Copyrighted (fair use for research)

Mapping:
Level 1 â†’ A1
Level 2 â†’ A2
Level 3 â†’ B1
Level 4 â†’ B2
Level 5 â†’ C1
Level 6 â†’ C2

Pros:
âœ… Perfect CEFR alignment
âœ… Engaging stories
âœ… Controlled vocabulary

Cons:
âš ï¸ Copyright issues
âš ï¸ Need to digitize
âš ï¸ May need permission
```

---

#### 8. **Project Gutenberg + Readability Scores**
```
URL: https://www.gutenberg.org/
Size: 70,000+ books
Labels: None (need to compute)
Format: Plain text, EPUB
License: Public domain (FREE)

Strategy:
1. Download books
2. Extract passages (500 words)
3. Compute readability (Flesch-Kincaid)
4. Map to CEFR
5. Manual verification

Pros:
âœ… Huge, free resource
âœ… Classic literature
âœ… High-quality writing

Cons:
âš ï¸ Need to label yourself
âš ï¸ Time-consuming
âš ï¸ Older English style
```

---

### **Tier 3: RESEARCH DATASETS** â­â­â­

#### 9. **Academic Papers with Annotations**
```
Sources:
- Papers With Code (https://paperswithcode.com/)
- ACL Anthology (https://aclanthology.org/)
- Shared Tasks (SemEval, CLEF)

Search keywords:
- "readability assessment dataset"
- "text difficulty corpus"
- "CEFR labeled corpus"
- "graded reading materials"

Pros:
âœ… Research-quality
âœ… Often pre-split
âœ… Benchmarks available

Cons:
âš ï¸ Smaller sizes
âš ï¸ Specific domains
âš ï¸ May need to request
```

---

## ğŸ“‹ Dataset Evaluation Checklist

### Before Using a Dataset

```
Size & Balance:
[ ] â‰¥ 1,000 total samples
[ ] â‰¥ 100 samples per CEFR level
[ ] Not too imbalanced (max 3:1 ratio)

Labels:
[ ] CEFR levels OR convertible (Lexile, Grade)
[ ] Labels are reliable (human-annotated preferred)
[ ] Label distribution documented

Quality:
[ ] Natural English text
[ ] No major OCR errors
[ ] Complete sentences
[ ] Appropriate length (50-500 words)

Diversity:
[ ] Multiple topics/genres
[ ] Not single-domain
[ ] Various text types

Licensing:
[ ] Free for academic use
[ ] No copyright violations
[ ] Proper attribution possible

Format:
[ ] Machine-readable (JSON, CSV, TXT)
[ ] Consistent structure
[ ] UTF-8 encoding
```

---

## ğŸ› ï¸ DIY Dataset Creation

### If you can't find good dataset...

#### **Option 1: Web Scraping (Legal sources)**

```python
# Example: Scrape graded readers from educational sites
Sources:
- ESL/EFL learning websites
- Educational publishers (with permission)
- Open educational resources

Tools:
- BeautifulSoup (Python)
- Scrapy
- Selenium (for JS-heavy sites)

Legal considerations:
âœ… Check robots.txt
âœ… Respect rate limits
âœ… Only scrape public content
âœ… Academic/research use
```

---

#### **Option 2: Manual Labeling**

```
Process:
1. Collect unlabeled texts (Project Gutenberg, news)
2. Use readability formulas (Flesch-Kincaid)
3. Manual review by English teachers
4. Inter-annotator agreement (â‰¥80%)

Tools:
- Flesch-Kincaid calculator
- Textstat (Python library)
- Readable.com

Time estimate: 100 texts/hour with tools
```

---

#### **Option 3: Data Augmentation**

```python
# Expand small dataset
Techniques:
1. Back-translation (ENâ†’FRâ†’EN)
2. Paraphrasing (with GPT)
3. Sentence reordering
4. Synonym replacement (at appropriate level)

Example:
Original (B1): "The government implemented new policies."
Augmented (B1): "New policies were implemented by the government."

Caution: Validate augmented samples!
```

---

## ğŸ“Š Recommended Starting Point

### **For File2Learning Project:**

#### **Phase 1: Quick Start (Week 1)**
```
Dataset: OneStopEnglish (567 samples)
+ Synthetic data (current 1,203 samples)
= TOTAL: ~1,770 samples

Expected accuracy: 75-80%
Good enough for: MVP, demo, proof of concept
```

#### **Phase 2: Improve Quality (Week 2-3)**
```
Add: NewsELA (request access)
Add: CommonLit (scrape 1,000 passages)
+ Manual labeling (500 samples from various sources)
= TOTAL: ~3,500 samples

Expected accuracy: 85-87%
Good for: Production, presentation
```

#### **Phase 3: Production Quality (Month 2)**
```
Add: Cambridge Corpus (if access granted)
Add: Manually curated examples (1,000+)
Quality control: Remove low-quality samples
= TOTAL: 5,000-10,000 samples

Expected accuracy: 88-92%
Good for: Publication, commercial use
```

---

## ğŸ¯ Action Plan for You

### **This Week:**

1. **Download OneStopEnglish** (30 minutes)
   ```bash
   git clone https://github.com/nishkalavallabhi/OneStopEnglishCorpus.git
   ```

2. **Process into JSON** (1 hour)
   - Extract texts from folders
   - Label: Eleâ†’A2, Intâ†’B1, Advâ†’C1
   - Combine with current dataset

3. **Request NewsELA access** (5 minutes)
   - Fill form at newsela.com/data
   - Wait 1-2 weeks for approval

4. **Scrape CommonLit** (2-3 hours)
   - 100-200 passages
   - Grade levels â†’ CEFR

### **Target:** 2,000-2,500 samples by end of week

---

## âš ï¸ Common Pitfalls to Avoid

```
âŒ DON'T:
1. Use only synthetic data (overfitting)
2. Mix languages (English + others)
3. Include code/formulas in text samples
4. Use auto-translated texts (quality issues)
5. Violate copyrights (paid content)
6. Ignore class imbalance
7. Use texts > 1000 words (hard to label)
8. Mix different labeling standards

âœ… DO:
1. Verify label quality (sample check)
2. Balance dataset across levels
3. Include diverse topics
4. Clean text (remove HTML, special chars)
5. Document sources & licenses
6. Split train/val/test properly
7. Manual QA on random samples
8. Track dataset version
```

---

## ğŸ“š Resources & Tools

### **Readability Tools:**
```
Python libraries:
- textstat (Flesch-Kincaid, Gunning Fog, etc.)
- nltk (sentence tokenization)
- spacy (NLP features)

Online calculators:
- readable.com
- readabilityformulas.com
- hemingwayapp.com
```

### **Dataset Processing:**
```python
# Install tools
pip install textstat nltk spacy beautifulsoup4 pandas

# Compute readability
import textstat
text = "Your English text here..."
fk_grade = textstat.flesch_kincaid_grade(text)
flesch_score = textstat.flesch_reading_ease(text)

# Map to CEFR
if fk_grade < 3: level = "A1"
elif fk_grade < 5: level = "A2"
elif fk_grade < 7: level = "B1"
elif fk_grade < 9: level = "B2"
elif fk_grade < 11: level = "C1"
else: level = "C2"
```

---

## ğŸ“ Academic Standards

If publishing results:
```
Report dataset details:
- Total samples & distribution
- Source & collection method
- Train/val/test split
- Inter-annotator agreement (if manual)
- Baseline metrics (Flesch-Kincaid)
- Any preprocessing applied

Cite properly:
- OneStopEnglish: Vajjala & LuÄiÄ‡ (2018)
- NewsELA: Xu et al. (2015)
- Cambridge Corpus: Cambridge Assessment
```

---

## âœ… Summary Checklist

**Minimum viable dataset:**
- [x] 1,000+ samples total
- [x] All 6 CEFR levels represented
- [x] Balanced distribution (150+ per level)
- [x] Natural English text
- [x] Multiple topics/genres
- [x] Clean formatting
- [x] Proper train/val/test split
- [x] Legal to use (free license)

**Start here:** OneStopEnglish + Current synthetic data = ~1,700 samples âœ…

**Good enough for:**
- âœ… Training working model
- âœ… Academic project presentation
- âœ… Proof of concept
- âœ… MVP deployment

---

## ğŸš€ Next Steps

1. **Download OneStopEnglish** â†’ process into JSON
2. **Train with combined dataset** â†’ see improvement
3. **While training:** Request NewsELA, scrape CommonLit
4. **Week 2:** Retrain with larger dataset
5. **Compare results:** Show improvement trajectory

---

**Questions?** Check specific dataset documentation or ask team!

Good luck hunting datasets! ğŸ¯


