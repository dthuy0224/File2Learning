# üìä Dataset Collection Guide - Difficulty Classification

Ti√™u ch√≠ v√† h∆∞·ªõng d·∫´n t√¨m dataset ch·∫•t l∆∞·ª£ng cho training AI model

---

## üéØ Ti√™u Ch√≠ Dataset Ch·∫•t L∆∞·ª£ng

### 1. **Minimum Requirements (B·∫ÆT BU·ªòC)**

#### ‚úÖ Size (K√≠ch th∆∞·ªõc)
```
Minimum:  1,000 samples  (c√≥ th·ªÉ train, accuracy ~75%)
Good:     5,000 samples  (accuracy ~85%)
Excellent: 10,000+ samples (accuracy ~90%+)

Per-label distribution:
- A1: ‚â• 150 samples
- A2: ‚â• 150 samples  
- B1: ‚â• 200 samples (most common level)
- B2: ‚â• 200 samples
- C1: ‚â• 150 samples
- C2: ‚â• 150 samples
```

**Why?** 
- BERT models c·∫ßn √≠t nh·∫•t 100 examples/class ƒë·ªÉ h·ªçc patterns
- Balanced dataset ‚Üí better generalization

---

#### ‚úÖ Labels (Nh√£n)
```
REQUIRED: CEFR levels (A1, A2, B1, B2, C1, C2)

Acceptable alternatives:
- Readability scores (Flesch-Kincaid) ‚Üí convert to CEFR
- Grade levels (Grade 1-12) ‚Üí map to CEFR
- Lexile scores (200L-1700L) ‚Üí convert to CEFR

AVOID:
‚ùå "Easy/Medium/Hard" (too vague)
‚ùå Unlabeled data (c·∫ßn manual labeling)
‚ùå Domain-specific only (medical, legal) ‚Üí not general
```

**Conversion table:**
```
Grade Level ‚Üí CEFR
1-2    ‚Üí A1
3-4    ‚Üí A2
5-6    ‚Üí B1
7-8    ‚Üí B2
9-10   ‚Üí C1
11-12  ‚Üí C2

Lexile ‚Üí CEFR
200-400L   ‚Üí A1
400-600L   ‚Üí A2
600-900L   ‚Üí B1
900-1100L  ‚Üí B2
1100-1300L ‚Üí C1
1300L+     ‚Üí C2
```

---

#### ‚úÖ Quality (Ch·∫•t l∆∞·ª£ng content)
```
Good text characteristics:
‚úÖ Natural English (not machine-translated)
‚úÖ Complete sentences
‚úÖ Proper grammar
‚úÖ Clear meaning
‚úÖ Appropriate length (50-500 words/sample)

Red flags:
‚ùå OCR errors (scanned documents with typos)
‚ùå Code-switching (mixed languages)
‚ùå Too short (< 20 words)
‚ùå Too long (> 1000 words) ‚Üí hard to label
‚ùå Machine-generated gibberish
```

---

#### ‚úÖ Diversity (ƒêa d·∫°ng)
```
Topic diversity:
‚úÖ News articles
‚úÖ Stories/narratives
‚úÖ Educational texts
‚úÖ Social media (informal)
‚úÖ Academic papers (formal)
‚úÖ Conversations/dialogues

Genre balance:
- Fiction: 30%
- Non-fiction: 40%
- Academic: 20%
- Informal: 10%

AVOID: Single-domain datasets (all medical, all legal)
```

---

### 2. **Nice-to-Have Features (KHUY·∫æN NGH·ªä)**

#### üåü Metadata
```
Useful metadata fields:
- Source (news, textbook, social media)
- Topic/category
- Author information
- Publication date
- Word count
- Readability metrics
- Target audience (kids, adults, professionals)
```

#### üåü Pre-processed
```
Ideal format:
{
  "text": "The actual English text...",
  "label": "B1",
  "metadata": {
    "source": "news",
    "topic": "environment",
    "word_count": 250,
    "flesch_score": 65.2
  }
}

Saves time vs raw text files!
```

#### üåü Train/Val/Test Splits
```
Pre-split datasets = time saver!

Look for:
- train.json
- val.json  
- test.json

With stratified sampling (balanced labels)
```

---

## üîç Recommended Dataset Sources

### **Tier 1: FREE & HIGH-QUALITY** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

#### 1. **OneStopEnglish Corpus**
```
URL: https://github.com/nishkalavallabhi/OneStopEnglishCorpus
Size: 567 texts (189 articles √ó 3 levels)
Labels: Elementary (A2), Intermediate (B1), Advanced (C1)
Format: Text files
License: CC BY-SA 4.0 (Free for academic)

Pros:
‚úÖ Same article at 3 difficulty levels
‚úÖ News articles (Guardian newspaper)
‚úÖ High quality, manually adapted
‚úÖ Academic research standard

Cons:
‚ö†Ô∏è Only 3 levels (missing A1, B2, C2)
‚ö†Ô∏è All news domain
‚ö†Ô∏è Need manual processing

Download:
git clone https://github.com/nishkalavallabhi/OneStopEnglishCorpus.git
```

---

#### 2. **NewsELA Dataset**
```
URL: https://newsela.com/data/
Size: 1,000+ articles √ó 5 levels
Labels: Lexile scores ‚Üí convert to CEFR
Format: HTML/Text
License: Academic use (free with request)

Pros:
‚úÖ 5 reading levels
‚úÖ Large scale
‚úÖ Current news topics
‚úÖ Used in research

Cons:
‚ö†Ô∏è Need to request access
‚ö†Ô∏è Web scraping required
‚ö†Ô∏è Lexile ‚Üí CEFR conversion needed

Access: Fill form at newsela.com/data
```

---

#### 3. **Cambridge English Corpus**
```
URL: https://www.cambridge.org/gb/cambridgeenglish/
Size: 5,000+ texts with CEFR labels
Labels: Official CEFR (A1-C2) ‚úÖ
Format: Various
License: Academic/Research license

Pros:
‚úÖ OFFICIAL CEFR labels (gold standard)
‚úÖ All 6 levels
‚úÖ Exam-quality texts
‚úÖ Multiple genres

Cons:
‚ö†Ô∏è Need to apply for access
‚ö†Ô∏è May have restrictions
‚ö†Ô∏è Not all publicly available

Contact: Cambridge Assessment English
```

---

#### 4. **CEFR-J Corpus (Japanese Learners)**
```
URL: http://www.cefr-j.org/
Size: 2,000+ texts
Labels: CEFR (A1-C2)
Format: Text files
License: Research use

Pros:
‚úÖ CEFR labeled
‚úÖ Educational focus
‚úÖ Free for research

Cons:
‚ö†Ô∏è Focused on Japanese learners
‚ö†Ô∏è May need translation
```

---

#### 5. **CommonLit Corpus**
```
URL: https://www.commonlit.org/
Size: 2,000+ reading passages
Labels: Grade levels (3-12) ‚Üí convert to CEFR
Format: Web (can scrape)
License: Educational use

Pros:
‚úÖ High-quality educational texts
‚úÖ Various genres
‚úÖ Comprehension questions included
‚úÖ Free for teachers

Cons:
‚ö†Ô∏è Need to scrape/download
‚ö†Ô∏è Grade level ‚Üí CEFR conversion
```

---

### **Tier 2: GOOD ALTERNATIVES** ‚≠ê‚≠ê‚≠ê‚≠ê

#### 6. **Wikipedia Simple English**
```
URL: https://simple.wikipedia.org/
Size: 200,000+ articles
Labels: Binary (Simple vs Normal)
Format: Wiki dumps
License: CC (Free)

Pros:
‚úÖ Huge dataset
‚úÖ Free & accessible
‚úÖ Easy to download

Cons:
‚ö†Ô∏è Only 2 levels (not full CEFR)
‚ö†Ô∏è Need manual CEFR labeling
‚ö†Ô∏è Quality varies

Use case: Augment dataset with A2 (simple) and C1 (normal)
```

---

#### 7. **Oxford Graded Readers**
```
URL: Various bookstores/libraries
Size: 1,000+ graded books
Labels: Oxford Bookworms levels (1-6)
Format: Books
License: Copyrighted (fair use for research)

Mapping:
Level 1 ‚Üí A1
Level 2 ‚Üí A2
Level 3 ‚Üí B1
Level 4 ‚Üí B2
Level 5 ‚Üí C1
Level 6 ‚Üí C2

Pros:
‚úÖ Perfect CEFR alignment
‚úÖ Engaging stories
‚úÖ Controlled vocabulary

Cons:
‚ö†Ô∏è Copyright issues
‚ö†Ô∏è Need to digitize
‚ö†Ô∏è May need permission
```

---

#### 8. **Project Gutenberg + Readability Scores**
```
URL: https://www.gutenberg.org/
Size: 70,000+ books
Labels: None (need to compute)
Format: Plain text, EPUB
License: Public domain (FREE)

Strategy:
1. Download books
2. Extract passages (500 words)
3. Compute readability (Flesch-Kincaid)
4. Map to CEFR
5. Manual verification

Pros:
‚úÖ Huge, free resource
‚úÖ Classic literature
‚úÖ High-quality writing

Cons:
‚ö†Ô∏è Need to label yourself
‚ö†Ô∏è Time-consuming
‚ö†Ô∏è Older English style
```

---

### **Tier 3: RESEARCH DATASETS** ‚≠ê‚≠ê‚≠ê

#### 9. **Academic Papers with Annotations**
```
Sources:
- Papers With Code (https://paperswithcode.com/)
- ACL Anthology (https://aclanthology.org/)
- Shared Tasks (SemEval, CLEF)

Search keywords:
- "readability assessment dataset"
- "text difficulty corpus"
- "CEFR labeled corpus"
- "graded reading materials"

Pros:
‚úÖ Research-quality
‚úÖ Often pre-split
‚úÖ Benchmarks available

Cons:
‚ö†Ô∏è Smaller sizes
‚ö†Ô∏è Specific domains
‚ö†Ô∏è May need to request
```

---

## üìã Dataset Evaluation Checklist

### Before Using a Dataset

```
Size & Balance:
[ ] ‚â• 1,000 total samples
[ ] ‚â• 100 samples per CEFR level
[ ] Not too imbalanced (max 3:1 ratio)

Labels:
[ ] CEFR levels OR convertible (Lexile, Grade)
[ ] Labels are reliable (human-annotated preferred)
[ ] Label distribution documented

Quality:
[ ] Natural English text
[ ] No major OCR errors
[ ] Complete sentences
[ ] Appropriate length (50-500 words)

Diversity:
[ ] Multiple topics/genres
[ ] Not single-domain
[ ] Various text types

Licensing:
[ ] Free for academic use
[ ] No copyright violations
[ ] Proper attribution possible

Format:
[ ] Machine-readable (JSON, CSV, TXT)
[ ] Consistent structure
[ ] UTF-8 encoding
```

---

## üõ†Ô∏è DIY Dataset Creation

### If you can't find good dataset...

#### **Option 1: Web Scraping (Legal sources)**

```python
# Example: Scrape graded readers from educational sites
Sources:
- ESL/EFL learning websites
- Educational publishers (with permission)
- Open educational resources

Tools:
- BeautifulSoup (Python)
- Scrapy
- Selenium (for JS-heavy sites)

Legal considerations:
‚úÖ Check robots.txt
‚úÖ Respect rate limits
‚úÖ Only scrape public content
‚úÖ Academic/research use
```

---

#### **Option 2: Manual Labeling**

```
Process:
1. Collect unlabeled texts (Project Gutenberg, news)
2. Use readability formulas (Flesch-Kincaid)
3. Manual review by English teachers
4. Inter-annotator agreement (‚â•80%)

Tools:
- Flesch-Kincaid calculator
- Textstat (Python library)
- Readable.com

Time estimate: 100 texts/hour with tools
```

---

#### **Option 3: Data Augmentation**

```python
# Expand small dataset
Techniques:
1. Back-translation (EN‚ÜíFR‚ÜíEN)
2. Paraphrasing (with GPT)
3. Sentence reordering
4. Synonym replacement (at appropriate level)

Example:
Original (B1): "The government implemented new policies."
Augmented (B1): "New policies were implemented by the government."

Caution: Validate augmented samples!
```

---

## üìä Recommended Starting Point

### **For File2Learning Project:**

#### **Phase 1: Quick Start (Week 1)**
```
Dataset: OneStopEnglish (567 samples)
+ Synthetic data (current 1,203 samples)
= TOTAL: ~1,770 samples

Expected accuracy: 75-80%
Good enough for: MVP, demo, proof of concept
```

#### **Phase 2: Improve Quality (Week 2-3)**
```
Add: NewsELA (request access)
Add: CommonLit (scrape 1,000 passages)
+ Manual labeling (500 samples from various sources)
= TOTAL: ~3,500 samples

Expected accuracy: 85-87%
Good for: Production, presentation
```

#### **Phase 3: Production Quality (Month 2)**
```
Add: Cambridge Corpus (if access granted)
Add: Manually curated examples (1,000+)
Quality control: Remove low-quality samples
= TOTAL: 5,000-10,000 samples

Expected accuracy: 88-92%
Good for: Publication, commercial use
```

---

## üéØ Action Plan for You

### **This Week:**

1. **Download OneStopEnglish** (30 minutes)
   ```bash
   git clone https://github.com/nishkalavallabhi/OneStopEnglishCorpus.git
   ```

2. **Process into JSON** (1 hour)
   - Extract texts from folders
   - Label: Ele‚ÜíA2, Int‚ÜíB1, Adv‚ÜíC1
   - Combine with current dataset

3. **Request NewsELA access** (5 minutes)
   - Fill form at newsela.com/data
   - Wait 1-2 weeks for approval

4. **Scrape CommonLit** (2-3 hours)
   - 100-200 passages
   - Grade levels ‚Üí CEFR

### **Target:** 2,000-2,500 samples by end of week

---

## ‚ö†Ô∏è Common Pitfalls to Avoid

```
‚ùå DON'T:
1. Use only synthetic data (overfitting)
2. Mix languages (English + others)
3. Include code/formulas in text samples
4. Use auto-translated texts (quality issues)
5. Violate copyrights (paid content)
6. Ignore class imbalance
7. Use texts > 1000 words (hard to label)
8. Mix different labeling standards

‚úÖ DO:
1. Verify label quality (sample check)
2. Balance dataset across levels
3. Include diverse topics
4. Clean text (remove HTML, special chars)
5. Document sources & licenses
6. Split train/val/test properly
7. Manual QA on random samples
8. Track dataset version
```

---

## üìö Resources & Tools

### **Readability Tools:**
```
Python libraries:
- textstat (Flesch-Kincaid, Gunning Fog, etc.)
- nltk (sentence tokenization)
- spacy (NLP features)

Online calculators:
- readable.com
- readabilityformulas.com
- hemingwayapp.com
```

### **Dataset Processing:**
```python
# Install tools
pip install textstat nltk spacy beautifulsoup4 pandas

# Compute readability
import textstat
text = "Your English text here..."
fk_grade = textstat.flesch_kincaid_grade(text)
flesch_score = textstat.flesch_reading_ease(text)

# Map to CEFR
if fk_grade < 3: level = "A1"
elif fk_grade < 5: level = "A2"
elif fk_grade < 7: level = "B1"
elif fk_grade < 9: level = "B2"
elif fk_grade < 11: level = "C1"
else: level = "C2"
```

---

## üéì Academic Standards

If publishing results:
```
Report dataset details:
- Total samples & distribution
- Source & collection method
- Train/val/test split
- Inter-annotator agreement (if manual)
- Baseline metrics (Flesch-Kincaid)
- Any preprocessing applied

Cite properly:
- OneStopEnglish: Vajjala & Luƒçiƒá (2018)
- NewsELA: Xu et al. (2015)
- Cambridge Corpus: Cambridge Assessment
```

---

## ‚úÖ Summary Checklist

**Minimum viable dataset:**
- [x] 1,000+ samples total
- [x] All 6 CEFR levels represented
- [x] Balanced distribution (150+ per level)
- [x] Natural English text
- [x] Multiple topics/genres
- [x] Clean formatting
- [x] Proper train/val/test split
- [x] Legal to use (free license)

**Start here:** OneStopEnglish + Current synthetic data = ~1,700 samples ‚úÖ

**Good enough for:**
- ‚úÖ Training working model
- ‚úÖ Academic project presentation
- ‚úÖ Proof of concept
- ‚úÖ MVP deployment

---

## üöÄ Next Steps

1. **Download OneStopEnglish** ‚Üí process into JSON
2. **Train with combined dataset** ‚Üí see improvement
3. **While training:** Request NewsELA, scrape CommonLit
4. **Week 2:** Retrain with larger dataset
5. **Compare results:** Show improvement trajectory

---

**Questions?** Check specific dataset documentation or ask team!

Good luck hunting datasets! üéØ


