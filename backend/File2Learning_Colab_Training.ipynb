{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì File2Learning - AI Model Training on Google Colab\n",
        "\n",
        "## üìö Difficulty Classifier Training Pipeline\n",
        "\n",
        "**Model**: DistilBERT-based Text Difficulty Classifier (A1-C2 CEFR levels)\n",
        "\n",
        "**GPU**: Tesla T4 (16GB VRAM) - Mi·ªÖn ph√≠ tr√™n Google Colab\n",
        "\n",
        "**Training Time**: ~8-12 ph√∫t\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Quick Start Guide:\n",
        "1. **Runtime** ‚Üí **Change runtime type** ‚Üí **GPU** (T4 ho·∫∑c V100)\n",
        "2. **Run All** (Runtime ‚Üí Run all) ho·∫∑c ch·∫°y t·ª´ng cell\n",
        "3. ƒê·ª£i training ho√†n th√†nh (~10 ph√∫t)\n",
        "4. Download model v·ªÅ local\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 1: Setup Environment & GPU Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîç GPU Information\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(f\"üî¢ CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"üêç PyTorch Version: {torch.__version__}\")\n",
        "else:\n",
        "    print(\"‚ùå GPU NOT AVAILABLE!\")\n",
        "    print(\"‚ö†Ô∏è  Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 2: Mount Google Drive (Optional)\n",
        "\n",
        "**N·∫øu b·∫°n mu·ªën save model v√†o Google Drive**, uncomment v√† ch·∫°y cell n√†y:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Create output directory in Drive\n",
        "# DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/File2Learning_Models'\n",
        "# os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "# print(f\"‚úÖ Google Drive mounted! Models will be saved to: {DRIVE_OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Step 3: Upload Project Files\n",
        "\n",
        "**Ch·ªçn 1 trong 2 options:**\n",
        "\n",
        "### **Option A: Upload t·ª´ local** (Recommended)\n",
        "1. Zip to√†n b·ªô folder `backend/` th√†nh `backend.zip`\n",
        "2. Upload file zip v√† extract\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: Upload ZIP file\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "print(\"üì§ Upload backend.zip file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"üì¶ Extracting {filename}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/')\n",
        "        print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Change to backend directory\n",
        "%cd /content/backend\n",
        "!pwd\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Option B: Clone t·ª´ GitHub** (N·∫øu b·∫°n ƒë√£ push code l√™n GitHub)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Option B: Clone from GitHub\n",
        "# !git clone https://github.com/YOUR_USERNAME/File2Learning.git\n",
        "# %cd File2Learning/backend\n",
        "# !pwd\n",
        "# !ls -la\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 4: Install Dependencies\n",
        "\n",
        "Install t·∫•t c·∫£ packages c·∫ßn thi·∫øt cho AI training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì¶ Installing AI dependencies...\")\n",
        "print(\"‚è≥ This may take 2-3 minutes...\\n\")\n",
        "\n",
        "# Install core packages\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers==4.36.0 tokenizers==0.15.0\n",
        "!pip install -q accelerate==0.25.0\n",
        "!pip install -q pandas numpy scikit-learn\n",
        "!pip install -q matplotlib seaborn plotly\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"\\n‚úÖ All dependencies installed!\")\n",
        "\n",
        "# Verify installation\n",
        "import transformers\n",
        "import torch\n",
        "print(f\"\\nüìö Transformers version: {transformers.__version__}\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 5: Verify Project Structure\n",
        "\n",
        "Ki·ªÉm tra xem t·∫•t c·∫£ files c·∫ßn thi·∫øt ƒë√£ c√≥ ch∆∞a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üîç Verifying project structure...\\n\")\n",
        "\n",
        "required_files = [\n",
        "    'train_ai_model.py',\n",
        "    'app/ai/models/difficulty_classifier.py',\n",
        "    'app/ai/training/train_difficulty.py',\n",
        "    'app/ai/datasets/collect_data.py',\n",
        "    'app/ai/utils/data_preprocessing.py',\n",
        "]\n",
        "\n",
        "all_good = True\n",
        "for file in required_files:\n",
        "    if Path(file).exists():\n",
        "        print(f\"‚úÖ {file}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file} - MISSING!\")\n",
        "        all_good = False\n",
        "\n",
        "if all_good:\n",
        "    print(\"\\nüéâ All required files present!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some files are missing. Please check your upload.\")\n",
        "\n",
        "# Check if dataset exists\n",
        "dataset_path = Path('app/ai/datasets/raw_dataset.json')\n",
        "if dataset_path.exists():\n",
        "    import json\n",
        "    with open(dataset_path) as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"\\nüìä Dataset found: {data.get('num_samples', 0)} samples\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Dataset not found. Will generate synthetic dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Step 6: Training Configuration\n",
        "\n",
        "C·∫•u h√¨nh t·ªëi ∆∞u cho GPU T4 (16GB VRAM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration for Google Colab T4\n",
        "TRAINING_CONFIG = {\n",
        "    'batch_size': 16,        # TƒÉng t·ª´ 8 (local) l√™n 16 v√¨ T4 c√≥ 16GB VRAM\n",
        "    'num_epochs': 3,         # Gi·ªØ nguy√™n\n",
        "    'learning_rate': 2e-5,   # Gi·ªØ nguy√™n\n",
        "    'max_length': 512,       # Gi·ªØ nguy√™n\n",
        "    'warmup_steps': 500,     # Gi·ªØ nguy√™n\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "print(\"‚öôÔ∏è Training Configuration for Google Colab\")\n",
        "print(\"=\"*70)\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"  {key:20s}: {value}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 7: Collect Training Data\n",
        "\n",
        "Generate synthetic dataset (ho·∫∑c s·ª≠ d·ª•ng dataset c√≥ s·∫µn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Step 7: Collecting training data...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python -m app.ai.datasets.collect_data\n",
        "\n",
        "print(\"\\n‚úÖ Data collection complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 8: Train the Model!\n",
        "\n",
        "**Main training process** - ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t!\n",
        "\n",
        "Expected time: **~8-12 ph√∫t** tr√™n T4 GPU\n",
        "\n",
        "### What happens:\n",
        "1. Load dataset v√† preprocessing\n",
        "2. Initialize DistilBERT model\n",
        "3. Train for 3 epochs\n",
        "4. Save best model d·ª±a tr√™n validation F1 score\n",
        "5. Generate training curves v√† confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"üöÄ Starting AI Model Training...\")\n",
        "print(\"=\"*70)\n",
        "print(\"‚è±Ô∏è  Estimated time: 8-12 minutes on T4 GPU\")\n",
        "print(\"üìä You'll see progress bars for each epoch\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Run training\n",
        "!python -m app.ai.training.train_difficulty\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"‚úÖ Training Complete!\")\n",
        "print(f\"‚è±Ô∏è  Total time: {duration/60:.2f} minutes ({duration:.0f} seconds)\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Step 9: View Training Results\n",
        "\n",
        "Visualize training curves v√† confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "print(\"üìà Training Results Visualization\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Display training curves\n",
        "curves_path = 'models/difficulty_classifier/training_curves.png'\n",
        "if os.path.exists(curves_path):\n",
        "    print(\"\\nüìä Training Curves:\")\n",
        "    display(Image(filename=curves_path))\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Training curves not found at {curves_path}\")\n",
        "\n",
        "# Display confusion matrix\n",
        "cm_path = 'models/difficulty_classifier/confusion_matrix.png'\n",
        "if os.path.exists(cm_path):\n",
        "    print(\"\\nüéØ Confusion Matrix:\")\n",
        "    display(Image(filename=cm_path))\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Confusion matrix not found at {cm_path}\")\n",
        "\n",
        "# List all generated files\n",
        "print(\"\\nüìÇ Generated Files:\")\n",
        "!ls -lh models/difficulty_classifier/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 10: Test Model Inference\n",
        "\n",
        "Test model v·ªõi m·ªôt s·ªë sample texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Import model class\n",
        "sys.path.append(str(Path.cwd()))\n",
        "from app.ai.models.difficulty_classifier import DifficultyClassifier\n",
        "\n",
        "print(\"üß™ Testing Model Inference\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_path = 'models/difficulty_classifier/best_model.pt'\n",
        "\n",
        "print(f\"üì• Loading model from {model_path}...\")\n",
        "model = DifficultyClassifier.load_model(model_path, device=device)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "print(\"‚úÖ Model loaded!\\n\")\n",
        "\n",
        "# Test samples\n",
        "test_texts = [\n",
        "    \"I have a cat. It is black.\",  # A1\n",
        "    \"Last week I went to the park. The weather was nice.\",  # A2\n",
        "    \"Learning a new language requires dedication and consistent practice.\",  # B1\n",
        "    \"The implementation of new technologies has fundamentally transformed businesses.\",  # B2\n",
        "    \"The paradigmatic shift in environmental policy necessitates comprehensive reevaluation.\",  # C1\n",
        "    \"The epistemological implications fundamentally challenge deterministic paradigms.\",  # C2\n",
        "]\n",
        "\n",
        "print(\"üîç Testing sample texts:\\n\")\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    # Tokenize\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    \n",
        "    # Predict\n",
        "    result = model.predict_text(input_ids, attention_mask)\n",
        "    \n",
        "    print(f\"Text {i}: {text[:60]}...\")\n",
        "    print(f\"  ‚û°Ô∏è  Predicted: {result['level']} (Confidence: {result['confidence']:.2%})\")\n",
        "    print(f\"  üìä Top 3: {', '.join([f'{k}:{v:.1%}' for k, v in sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:3]])}\") \n",
        "    print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ Inference test complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 11: Download Trained Model\n",
        "\n",
        "Download model v√† results v·ªÅ m√°y local\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"üíæ Preparing files for download...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create zip file with all results\n",
        "output_dir = 'models/difficulty_classifier'\n",
        "zip_filename = 'file2learning_trained_model'\n",
        "\n",
        "# Zip the model directory\n",
        "shutil.make_archive(zip_filename, 'zip', output_dir)\n",
        "\n",
        "zip_file = f\"{zip_filename}.zip\"\n",
        "print(f\"\\nüì¶ Created {zip_file}\")\n",
        "print(\"\\nContents:\")\n",
        "!unzip -l {zip_file}\n",
        "\n",
        "print(\"\\n‚¨áÔ∏è  Downloading...\")\n",
        "files.download(zip_file)\n",
        "\n",
        "print(\"\\n‚úÖ Download complete!\")\n",
        "print(\"\\nüìã Next steps:\")\n",
        "print(\"  1. Extract the zip file\")\n",
        "print(\"  2. Copy contents to your local: backend/models/difficulty_classifier/\")\n",
        "print(\"  3. Test model tr√™n local project\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéâ Training Complete!\n",
        "\n",
        "### üìä Summary\n",
        "\n",
        "B·∫°n ƒë√£ successfully train **Difficulty Classifier** v·ªõi:\n",
        "- ‚úÖ Model: DistilBERT (66M parameters)\n",
        "- ‚úÖ Task: 6-class classification (A1, A2, B1, B2, C1, C2)\n",
        "- ‚úÖ GPU: Google Colab T4 (16GB VRAM)\n",
        "- ‚úÖ Dataset: Synthetic + OneStop English Corpus\n",
        "\n",
        "### üìÇ Output Files\n",
        "- `best_model.pt` - Trained model weights\n",
        "- `training_curves.png` - Loss/Accuracy/F1 curves\n",
        "- `confusion_matrix.png` - Model performance visualization\n",
        "- Checkpoint files for each epoch\n",
        "\n",
        "### üîÑ Next Steps\n",
        "1. Download model v·ªÅ local project\n",
        "2. Test model trong application\n",
        "3. Integrate v√†o document processing pipeline\n",
        "4. Fine-tune n·∫øu c·∫ßn v·ªõi real user data\n",
        "\n",
        "### üí° Tips\n",
        "- N·∫øu mu·ªën train l·∫°i v·ªõi parameters kh√°c, ch·ªânh config ·ªü **Step 6**\n",
        "- N·∫øu mu·ªën train v·ªõi dataset l·ªõn h∆°n, add more data v√†o `collect_data.py`\n",
        "- Model c√≥ th·ªÉ improve over time khi c√≥ real user data\n",
        "\n",
        "---\n",
        "\n",
        "### üìû Troubleshooting\n",
        "\n",
        "**Common Issues:**\n",
        "\n",
        "1. **GPU Not Available** ‚Üí Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "2. **Out of Memory** ‚Üí Gi·∫£m batch_size t·ª´ 16 xu·ªëng 8\n",
        "3. **Files Not Found** ‚Üí Ki·ªÉm tra l·∫°i upload ·ªü Step 3\n",
        "4. **Import Errors** ‚Üí Re-run Step 4 (Install dependencies)\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Training! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
