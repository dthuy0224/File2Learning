================================================================================
ğŸ““ FILE2LEARNING - GOOGLE COLAB NOTEBOOK (READY TO USE)
================================================================================

HÆ¯á»šNG DáºªN:
1. Má»Ÿ Google Colab: https://colab.research.google.com/
2. File â†’ New notebook
3. Copy tá»«ng cell dÆ°á»›i Ä‘Ã¢y vÃ o notebook má»›i
4. Run tuáº§n tá»±

LÆ¯U Ã QUAN TRá»ŒNG:
- Cell 2 (Numpy Fix) â†’ PHáº¢I RESTART RUNTIME sau khi cháº¡y
- Sau khi restart â†’ Skip Cell 2 â†’ Cháº¡y tá»« Cell 3 trá»Ÿ Ä‘i

================================================================================


# ============================= CELL 1 =====================================
# Type: Markdown
# Copy toÃ n bá»™ text dÆ°á»›i Ä‘Ã¢y vÃ o má»™t Markdown cell

# ğŸ“ File2Learning - AI Model Training on Google Colab

## ğŸ“š Difficulty Classifier Training Pipeline

**Model**: DistilBERT-based Text Difficulty Classifier (A1-C2 CEFR levels)

**GPU**: Tesla T4 (16GB VRAM) - Miá»…n phÃ­ trÃªn Google Colab

**Training Time**: ~8-12 phÃºt

---

### ğŸš€ Quick Start Guide:
1. **Runtime** â†’ **Change runtime type** â†’ **GPU** (T4 hoáº·c V100)
2. **Run All** (Runtime â†’ Run all) hoáº·c cháº¡y tá»«ng cell
3. **âš ï¸ QUAN TRá»ŒNG:** Sau Cell 2, pháº£i **RESTART RUNTIME**
4. Äá»£i training hoÃ n thÃ nh (~10 phÃºt)
5. Download model vá» local

---


# ============================= CELL 2 =====================================
# Type: Markdown

## ğŸ”§ Step 1: Environment Check & GPU Detection


# ============================= CELL 3 =====================================
# Type: Code

# Check GPU availability and Python environment
import torch
import os
import sys

print("="*70)
print("ğŸ” ENVIRONMENT CHECK")
print("="*70)

print(f"\nğŸ Python: {sys.version.split()[0]}")

if torch.cuda.is_available():
    print(f"\nâœ… GPU Available: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"ğŸ”¢ CUDA Version: {torch.version.cuda}")
    print(f"ğŸ”¥ PyTorch Version: {torch.__version__}")
else:
    print("\nâŒ GPU NOT AVAILABLE!")
    print("âš ï¸  Go to: Runtime â†’ Change runtime type â†’ GPU (T4)")
    print("âš ï¸  Training will be VERY slow on CPU")

print("\n" + "="*70)
print("âœ… Environment check complete!")
print("="*70)


# ============================= CELL 4 =====================================
# Type: Markdown

## ğŸ”§ Step 2: Fix Numpy Compatibility (CRITICAL!)

**âš ï¸ MUST RUN THIS CELL!**

This fixes the numpy compatibility issue with scikit-learn on Colab.

**AFTER RUNNING THIS CELL:**
1. Runtime â†’ Restart runtime
2. Run Cell 1 again
3. SKIP this Cell 2
4. Continue from Cell 3 onwards


# ============================= CELL 5 =====================================
# Type: Code

print("="*70)
print("ğŸ”§ FIXING NUMPY COMPATIBILITY")
print("="*70)

print("\nâš ï¸  Installing compatible versions...")
print("   - numpy 1.26.4")
print("   - scikit-learn 1.3.2")
print("   - scipy 1.11.4\n")

# Install compatible versions
!pip install -q --upgrade --no-cache-dir numpy==1.26.4 scikit-learn==1.3.2 scipy==1.11.4

print("\nâœ… Compatible versions installed!")

print("\n" + "="*70)
print("âš ï¸  IMPORTANT: RESTART RUNTIME NOW!")
print("="*70)
print("\nSteps:")
print("1. Runtime â†’ Restart runtime")
print("2. Run Cell 1 again (Environment Check)")
print("3. SKIP this Cell 2 (already done)")
print("4. Continue from Cell 3 onwards")
print("\n" + "="*70)


# ============================= CELL 6 =====================================
# Type: Markdown

## ğŸ“¦ Step 3: Verify Dependencies

**Run this AFTER restarting runtime**


# ============================= CELL 7 =====================================
# Type: Code

print("="*70)
print("ğŸ“¦ VERIFYING DEPENDENCIES")
print("="*70)

# Check critical packages
try:
    import numpy as np
    print(f"\nâœ… numpy: {np.__version__}")
    if not np.__version__.startswith('1.26'):
        print("âš ï¸  WARNING: numpy version may cause issues!")
        print("   Expected: 1.26.x")
except Exception as e:
    print(f"\nâŒ numpy error: {e}")
    print("âš ï¸ Did you restart runtime after Cell 2?")

try:
    import pandas as pd
    print(f"âœ… pandas: {pd.__version__}")
except Exception as e:
    print(f"âŒ pandas: {e}")

try:
    import sklearn
    print(f"âœ… scikit-learn: {sklearn.__version__}")
    if not sklearn.__version__.startswith('1.3'):
        print("âš ï¸  WARNING: scikit-learn version may cause issues!")
        print("   Expected: 1.3.x")
except Exception as e:
    print(f"âŒ scikit-learn: {e}")
    print("âš ï¸ Did you restart runtime after Cell 2?")

try:
    import torch
    print(f"âœ… PyTorch: {torch.__version__}")
    print(f"   CUDA: {torch.cuda.is_available()}")
except Exception as e:
    print(f"âŒ PyTorch: {e}")

print("\n" + "="*70)
print("âœ… Dependency check complete!")
print("="*70)


# ============================= CELL 8 =====================================
# Type: Markdown

## ğŸ“¦ Step 4: Install AI Training Dependencies

Install transformers vÃ  cÃ¡c packages cáº§n thiáº¿t cho training


# ============================= CELL 9 =====================================
# Type: Code

print("="*70)
print("ğŸ“¦ INSTALLING AI TRAINING DEPENDENCIES")
print("="*70)
print("â³ This may take 2-3 minutes...\n")

# Install transformers and related packages
!pip install -q transformers==4.36.0 tokenizers==0.15.0
!pip install -q accelerate==0.25.0
!pip install -q matplotlib seaborn plotly
!pip install -q tqdm

print("\nâœ… All AI dependencies installed!")

# Verify transformers installation
print("\n" + "="*70)
print("ğŸ” VERIFYING AI PACKAGES")
print("="*70)

try:
    import transformers
    print(f"\nâœ… Transformers: {transformers.__version__}")
    
    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
    print("âœ… DistilBERT models available")
    
except Exception as e:
    print(f"\nâŒ Transformers import error: {e}")
    print("âš ï¸ Training may fail!")

try:
    from tqdm import tqdm
    print("âœ… tqdm available")
except:
    print("âš ï¸ tqdm not available (not critical)")

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    print("âœ… Plotting libraries available")
except:
    print("âš ï¸ Plotting libraries not available (not critical)")

print("\n" + "="*70)
print("âœ… Ready for training!")
print("="*70)


# ============================= CELL 10 =====================================
# Type: Markdown

## ğŸ“ Step 5: Upload Project Files

**Upload backend.zip** (Ä‘Ã£ zip toÃ n bá»™ folder backend/)


# ============================= CELL 11 =====================================
# Type: Code

# Upload ZIP file
from google.colab import files
import zipfile

print("ğŸ“¤ Upload backend.zip file...")
print("â³ Waiting for file selection...\n")

uploaded = files.upload()

# Extract
for filename in uploaded.keys():
    if filename.endswith('.zip'):
        print(f"\nğŸ“¦ Extracting {filename}...")
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall('/content/')
        print("âœ… Extraction complete!")
        break

# Change to backend directory
print("\nğŸ“‚ Changing to backend directory...")
%cd /content/backend
print("\nğŸ“‹ Current directory contents:")
!ls -la

print("\nâœ… Project files ready!")


# ============================= CELL 12 =====================================
# Type: Markdown

## ğŸ” Step 6: Verify Project Structure

Kiá»ƒm tra xem táº¥t cáº£ files cáº§n thiáº¿t Ä‘Ã£ cÃ³ chÆ°a


# ============================= CELL 13 =====================================
# Type: Code

import os
from pathlib import Path

print("="*70)
print("ğŸ” VERIFYING PROJECT STRUCTURE")
print("="*70)

required_files = [
    'train_ai_model.py',
    'app/ai/models/difficulty_classifier.py',
    'app/ai/training/train_difficulty.py',
    'app/ai/datasets/collect_data.py',
    'app/ai/utils/data_preprocessing.py',
]

all_good = True
missing_files = []

for file in required_files:
    if Path(file).exists():
        print(f"âœ… {file}")
    else:
        print(f"âŒ {file} - MISSING!")
        missing_files.append(file)
        all_good = False

if all_good:
    print("\nğŸ‰ All required files present!")
else:
    print(f"\nâš ï¸  Missing {len(missing_files)} file(s):")
    for f in missing_files:
        print(f"   - {f}")
    print("\nâŒ Please check your upload in Step 5")

# Check if dataset exists
dataset_path = Path('app/ai/datasets/raw_dataset.json')
if dataset_path.exists():
    try:
        import json
        with open(dataset_path) as f:
            data = json.load(f)
        num_samples = data.get('num_samples', len(data.get('data', [])))
        print(f"\nğŸ“Š Dataset found: {num_samples} samples")
    except Exception as e:
        print(f"\nâš ï¸  Dataset file exists but couldn't read: {e}")
        print("   Will generate synthetic dataset")
else:
    print("\nâš ï¸  Dataset not found. Will generate synthetic dataset.")

print("\n" + "="*70)


# ============================= CELL 14 =====================================
# Type: Markdown

## âš™ï¸ Step 7: Training Configuration

Cáº¥u hÃ¬nh tá»‘i Æ°u cho GPU T4 (16GB VRAM)


# ============================= CELL 15 =====================================
# Type: Code

import torch

# Training configuration for Google Colab T4
TRAINING_CONFIG = {
    'batch_size': 16,        # TÄƒng tá»« 8 (local) lÃªn 16 vÃ¬ T4 cÃ³ 16GB VRAM
    'num_epochs': 3,         # Giá»¯ nguyÃªn
    'learning_rate': 2e-5,   # Giá»¯ nguyÃªn
    'max_length': 512,       # Giá»¯ nguyÃªn
    'warmup_steps': 500,     # Giá»¯ nguyÃªn
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

print("="*70)
print("âš™ï¸  TRAINING CONFIGURATION")
print("="*70)
for key, value in TRAINING_CONFIG.items():
    print(f"  {key:20s}: {value}")

if TRAINING_CONFIG['device'] == 'cpu':
    print("\nâš ï¸  WARNING: Training on CPU will be VERY slow!")
    print("   Consider enabling GPU: Runtime â†’ Change runtime type â†’ GPU")

print("="*70)


# ============================= CELL 16 =====================================
# Type: Markdown

## ğŸ“Š Step 8: Collect Training Data

Generate synthetic dataset (hoáº·c sá»­ dá»¥ng dataset cÃ³ sáºµn)


# ============================= CELL 17 =====================================
# Type: Code

print("="*70)
print("ğŸ“Š COLLECTING TRAINING DATA")
print("="*70)

try:
    # Try to run the data collection script
    print("\nâ³ Running data collection script...")
    !python -m app.ai.datasets.collect_data
    print("\nâœ… Data collection complete!")
    
except Exception as e:
    print(f"\nâš ï¸  Data collection script failed: {e}")
    print("   This is OK - training will generate synthetic data automatically")

print("\n" + "="*70)


# ============================= CELL 18 =====================================
# Type: Markdown

## ğŸš€ Step 9: Train the Model!

**Main training process** - ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng nháº¥t!

Expected time: **~8-12 phÃºt** trÃªn T4 GPU

### What happens:
1. Load dataset vÃ  preprocessing
2. Initialize DistilBERT model
3. Train for 3 epochs
4. Save best model dá»±a trÃªn validation F1 score
5. Generate training curves vÃ  confusion matrix


# ============================= CELL 19 =====================================
# Type: Code

import time

print("="*70)
print("ğŸš€ STARTING AI MODEL TRAINING")
print("="*70)
print("\nâ±ï¸  Estimated time: 8-12 minutes on T4 GPU")
print("ğŸ“Š You'll see progress bars for each epoch")
print("ğŸ’¾ Best model will be saved automatically")
print("\n" + "="*70)
print()

start_time = time.time()

try:
    # Run training
    !python -m app.ai.training.train_difficulty
    
    end_time = time.time()
    duration = end_time - start_time
    
    print("\n" + "="*70)
    print("âœ… TRAINING COMPLETE!")
    print("="*70)
    print(f"â±ï¸  Total time: {duration/60:.2f} minutes ({duration:.0f} seconds)")
    print("="*70)
    
except Exception as e:
    print("\n" + "="*70)
    print("âŒ TRAINING FAILED!")
    print("="*70)
    print(f"Error: {e}")
    print("\nğŸ’¡ Troubleshooting:")
    print("   1. Check if all files were uploaded correctly (Step 6)")
    print("   2. Verify dependencies are installed (Step 4)")
    print("   3. Make sure GPU is available (Step 1)")
    print("="*70)


# ============================= CELL 20 =====================================
# Type: Markdown

## ğŸ“ˆ Step 10: View Training Results

Visualize training curves vÃ  confusion matrix


# ============================= CELL 21 =====================================
# Type: Code

from IPython.display import Image, display
import os
from pathlib import Path

print("="*70)
print("ğŸ“ˆ TRAINING RESULTS VISUALIZATION")
print("="*70)

# Check for results directory
results_dirs = [
    'models/difficulty_classifier',
    'app/ai/models/trained',
    './results'
]

found_dir = None
for dir_path in results_dirs:
    if Path(dir_path).exists():
        found_dir = dir_path
        break

if found_dir:
    print(f"\nğŸ“‚ Results directory: {found_dir}")
    
    # Display training curves
    curves_path = Path(found_dir) / 'training_curves.png'
    if curves_path.exists():
        print("\nğŸ“Š Training Curves:")
        display(Image(filename=str(curves_path)))
    else:
        print(f"\nâš ï¸  Training curves not found")
    
    # Display confusion matrix
    cm_path = Path(found_dir) / 'confusion_matrix.png'
    if cm_path.exists():
        print("\nğŸ¯ Confusion Matrix:")
        display(Image(filename=str(cm_path)))
    else:
        print(f"\nâš ï¸  Confusion matrix not found")
    
    # List all files
    print("\nğŸ“‚ Generated Files:")
    !ls -lh {found_dir}
    
else:
    print("\nâš ï¸  Results directory not found!")
    print("   Training may have failed. Check Step 9 output.")

print("\n" + "="*70)


# ============================= CELL 22 =====================================
# Type: Markdown

## ğŸ§ª Step 11: Test Model Inference

Test model vá»›i má»™t sá»‘ sample texts


# ============================= CELL 23 =====================================
# Type: Code

import torch
from transformers import DistilBertTokenizer
import sys
from pathlib import Path

print("="*70)
print("ğŸ§ª TESTING MODEL INFERENCE")
print("="*70)

try:
    # Import model class
    sys.path.insert(0, str(Path.cwd()))
    from app.ai.models.difficulty_classifier import DifficultyClassifier
    
    # Find model file
    model_paths = [
        'models/difficulty_classifier/best_model.pt',
        'app/ai/models/trained/best_model.pt',
        './results/best_model.pt'
    ]
    
    model_path = None
    for path in model_paths:
        if Path(path).exists():
            model_path = path
            break
    
    if not model_path:
        raise FileNotFoundError("Model file not found!")
    
    # Load model
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nğŸ“¥ Loading model from: {model_path}")
    print(f"ğŸ® Device: {device}")
    
    model = DifficultyClassifier.load_model(model_path, device=device)
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    print("âœ… Model loaded successfully!\n")
    
    # Test samples
    test_texts = [
        ("I have a cat. It is black.", "A1"),
        ("Last week I went to the park. The weather was nice.", "A2"),
        ("Learning a new language requires dedication and practice.", "B1"),
        ("The implementation of new technologies has transformed businesses.", "B2"),
        ("The paradigmatic shift in policy necessitates comprehensive analysis.", "C1"),
        ("The epistemological implications challenge deterministic paradigms.", "C2"),
    ]
    
    print("="*70)
    print("ğŸ” TESTING SAMPLE TEXTS")
    print("="*70)
    
    correct = 0
    
    for i, (text, expected_level) in enumerate(test_texts, 1):
        # Tokenize
        encoding = tokenizer(
            text,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)
        
        # Predict
        result = model.predict_text(input_ids, attention_mask)
        predicted_level = result['level']
        
        # Check if correct (allow Â±1 level difference)
        level_order = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
        expected_idx = level_order.index(expected_level)
        predicted_idx = level_order.index(predicted_level)
        is_correct = abs(expected_idx - predicted_idx) <= 1
        
        if is_correct:
            correct += 1
            status = "âœ…"
        else:
            status = "âŒ"
        
        print(f"\n{status} Test {i}:")
        print(f"   Text: {text[:60]}...")
        print(f"   Expected: {expected_level} | Predicted: {predicted_level}")
        print(f"   Confidence: {result['confidence']:.2%}")
        
        # Show top 3 predictions
        top_3 = sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:3]
        print(f"   Top 3: {', '.join([f'{k}:{v:.1%}' for k, v in top_3])}")
    
    accuracy = correct / len(test_texts)
    print("\n" + "="*70)
    print(f"ğŸ“Š Test Accuracy: {accuracy:.1%} ({correct}/{len(test_texts)} within Â±1 level)")
    print("="*70)
    
    if accuracy >= 0.8:
        print("âœ… Model performing well!")
    elif accuracy >= 0.5:
        print("âš ï¸  Model is OK, but could be better")
    else:
        print("âŒ Model needs more training")
    
    print("\nâœ… Inference test complete!")
    
except Exception as e:
    print(f"\nâŒ INFERENCE TEST FAILED!")
    print(f"Error: {e}")
    print("\nğŸ’¡ This could mean:")
    print("   1. Model file not found (training may have failed)")
    print("   2. Model class import error")
    print("   3. GPU/memory issue")
    print("\nCheck previous steps for errors.")

print("\n" + "="*70)


# ============================= CELL 24 =====================================
# Type: Markdown

## ğŸ’¾ Step 12: Download Trained Model

Download model vÃ  results vá» mÃ¡y local


# ============================= CELL 25 =====================================
# Type: Code

from google.colab import files
import shutil
import os
from pathlib import Path

print("="*70)
print("ğŸ’¾ PREPARING FILES FOR DOWNLOAD")
print("="*70)

try:
    # Find output directory
    output_dirs = [
        'models/difficulty_classifier',
        'app/ai/models/trained',
        './results'
    ]
    
    output_dir = None
    for dir_path in output_dirs:
        if Path(dir_path).exists():
            output_dir = dir_path
            break
    
    if not output_dir:
        raise FileNotFoundError("Output directory not found!")
    
    print(f"\nğŸ“‚ Found output directory: {output_dir}")
    
    # Create zip file
    zip_filename = 'file2learning_trained_model'
    
    print(f"\nğŸ“¦ Creating {zip_filename}.zip...")
    shutil.make_archive(zip_filename, 'zip', output_dir)
    
    zip_file = f"{zip_filename}.zip"
    file_size = Path(zip_file).stat().st_size / 1024 / 1024  # MB
    
    print(f"âœ… Created {zip_file} ({file_size:.1f} MB)")
    
    print("\nğŸ“‹ Archive contents:")
    !unzip -l {zip_file}
    
    print("\nâ¬‡ï¸  Starting download...")
    files.download(zip_file)
    
    print("\nâœ… Download complete!")
    print("\n" + "="*70)
    print("ğŸ“‹ NEXT STEPS")
    print("="*70)
    print("1. Extract the zip file on your local machine")
    print("2. Copy contents to: backend/models/difficulty_classifier/")
    print("3. Test model in your local project")
    print("4. Integrate into document processing pipeline")
    print("="*70)
    
except Exception as e:
    print(f"\nâŒ DOWNLOAD PREPARATION FAILED!")
    print(f"Error: {e}")
    print("\nğŸ’¡ Manual download:")
    print("1. Look in left sidebar (Files icon)")
    print("2. Navigate to the model directory")
    print("3. Right-click files â†’ Download")
    print("\nKey files to download:")
    print("  - best_model.pt")
    print("  - training_curves.png")
    print("  - confusion_matrix.png")

print("\n" + "="*70)


# ============================= CELL 26 =====================================
# Type: Markdown

---

## ğŸ‰ Training Complete!

### ğŸ“Š Summary

Báº¡n Ä‘Ã£ successfully train **Difficulty Classifier** vá»›i:
- âœ… Model: DistilBERT (66M parameters)
- âœ… Task: 6-class classification (A1, A2, B1, B2, C1, C2)
- âœ… GPU: Google Colab T4 (16GB VRAM)
- âœ… Dataset: Synthetic + Real CEFR texts

### ğŸ“‚ Output Files
- `best_model.pt` - Trained model weights (~250 MB)
- `training_curves.png` - Loss/Accuracy/F1 visualization
- `confusion_matrix.png` - Performance heatmap
- Checkpoint files for each epoch

### ğŸš€ Next Steps

1. âœ… Download model (Step 12 above)
2. Extract ZIP file to `backend/models/difficulty_classifier/`
3. Test model in your local project
4. Integrate into document processing pipeline
5. Monitor accuracy with user feedback

---

**Happy Training! ğŸš€**

*File2Learning - Learn Smarter with AI*


================================================================================
âœ… NOTEBOOK HOÃ€N Táº¤T!
================================================================================

Tá»”NG Cá»˜NG: 26 CELLS
- Markdown cells: 13
- Code cells: 13

CÃCH Sá»¬ Dá»¤NG:
1. Má»Ÿ Google Colab má»›i
2. Copy tá»«ng cell theo thá»© tá»±
3. CHÃš Ã: Cell 5 (Numpy Fix) â†’ RESTART RUNTIME

TIMELINE Dá»° KIáº¾N:
- Setup: 5 phÃºt
- Upload files: 2 phÃºt  
- Training: 8-12 phÃºt
- Download: 2 phÃºt
â†’ Tá»”NG: ~20-25 phÃºt

================================================================================

